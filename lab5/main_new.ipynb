{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9597f12a-ff40-496f-b247-9d5983e506ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, SGD\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355199a9-bf1a-4ab0-8f2f-4fa73cfde80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 2 NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.cuda.set_device(2)\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614307e8-96e5-47dd-888d-3485331dfc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"wg4bpm33hj-2/images\"\n",
    "MASKS_PATH = \"wg4bpm33hj-2/masks\"\n",
    "WEIGHT_PATH = \"task18\"\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = 800\n",
    "LR = 0.001\n",
    "NUM_SAVE = 5\n",
    "NK = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75266428-fb7a-496b-90ce-f96e5ce883ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCAgTDataset(Dataset):\n",
    "    def __init__(self, image_path: str, label_path: str, set_type=\"train\", first_transform=None, sec_transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.first_transform = first_transform\n",
    "        self.sec_transform = sec_transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.split_ratio = {\"train\": (0.0, 0.7), \"val\": (0.7, 0.8), \"test\": (0.8, 1.0)}\n",
    "        self.maxV = 7\n",
    "        self.mask_ratio = 255 // self.maxV\n",
    "        \n",
    "        assert set_type in self.split_ratio.keys(), \"dataset type error\"\n",
    "  \n",
    "        for root, dirs, files in os.walk(image_path):\n",
    "            for f in files:\n",
    "                self.images.append(os.path.join(root, f))\n",
    "                \n",
    "        for root, dirs, files in os.walk(label_path):\n",
    "            for f in files:\n",
    "                self.labels.append(os.path.join(root, f))\n",
    "                \n",
    "        assert len(self.images) == len(self.labels), f\"data length error, {len(self.images)}, {len(self.labels)}\"\n",
    "        \n",
    "        for ele in zip(sorted(self.images), sorted(self.labels)):\n",
    "            self.data.append(ele)\n",
    "        \n",
    "        shuffle(self.data)\n",
    "        self.data = self.data[int(len(self.data)*self.split_ratio[set_type][0]) : int(len(self.data)*self.split_ratio[set_type][1])]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, mask_path = self.data[index]\n",
    "\n",
    "        #image = torch.from_numpy(np.array(Image.open(image_path), dtype=np.float64))\n",
    "        image = Image.open(image_path)\n",
    "        #mask =  Image.open(mask_path).convert(\"L\")\n",
    "        mask =  Image.open(mask_path)\n",
    "        \n",
    "        if self.first_transform is not None:\n",
    "            image = self.first_transform(image)\n",
    "            y = self.first_transform(mask)\n",
    "\n",
    "        image = transforms.ToTensor()(image)\n",
    "        y = np.array(y)\n",
    "        y = torch.from_numpy(y)\n",
    "        \n",
    "        y = y.type(torch.LongTensor)\n",
    "        \n",
    "        return image, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db9af62-446f-4189-8c30-8826cca4f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#         transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
    "#     ]) \n",
    "# trainset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"train\", first_transform=transform)\n",
    "# x, y = trainset[0]\n",
    "# print(x.shape, y.shape)\n",
    "# print(x)\n",
    "# print(torch.unique(y))\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e00ad95-dba7-4f99-a5b3-7e5d234fe5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = np.random.randint(0, 8, (8, 1, 3, 3))\n",
    "# z = torch.randint(0, 8, (8, 8, 3, 3)).float()\n",
    "# #y = torch.clone(z)\n",
    "# #print(y)\n",
    "# n = np.zeros((8, 8, 3, 3))\n",
    "\n",
    "# for i, row in enumerate(y):\n",
    "#     for j, ele in enumerate(row):\n",
    "#         print(ele)\n",
    "#         n[ele][i][j] = 1\n",
    "# print(n)\n",
    "\n",
    "# sf = nn.Softmax(1)\n",
    "# #print(sf(z))\n",
    "# print(z[0])\n",
    "# x = torch.argmax(z, axis=1).unsqueeze(1).float()\n",
    "# print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f4ac2c-cd04-4240-a71c-f5fb82c61783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "#         transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "#     ])\n",
    "# label_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "#     ])\n",
    "# trainset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"train\", image_transform=image_transform, label_transform=label_transform)\n",
    "# x, y = trainset[0]\n",
    "# y = y.int()\n",
    "# print(torch.max(y))\n",
    "# print(torch.min(y))\n",
    "# print(y.shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd55d0b-d3ae-4c17-be48-4b9a524a0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up_layer = nn.ConvTranspose2d(in_channels, out_channels, 2, 2)\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, keep):\n",
    "        x = self.up_layer(x)\n",
    "        x = torch.cat([x, keep], dim=1)\n",
    "        x = self.double_conv(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, hiddens=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList([Up(hidden*2, hidden) for hidden in hiddens[::-1]])\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        for hidden in hiddens:\n",
    "            self.downs.append(DoubleConv(in_channels, hidden))\n",
    "            in_channels = hidden\n",
    "            \n",
    "        #for hidden in hiddens[::-1]:\n",
    "        #    self.ups.append(Up(hidden*2, hidden))\n",
    "            \n",
    "        self.midden_layer = DoubleConv(hiddens[-1], hiddens[-1]*2)\n",
    "        self.out_layer = nn.Conv2d(hiddens[0], out_channels, 1)\n",
    "        #self.sofmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keeps = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            keeps.append(x)\n",
    "            x = self.pool(x)\n",
    "            \n",
    "        x = self.midden_layer(x)\n",
    "        keeps = keeps[::-1]\n",
    "        \n",
    "        for i, up in enumerate(self.ups):\n",
    "            x = up(x, keeps[i])\n",
    "            \n",
    "        x = self.out_layer(x)\n",
    "        #x = self.sofmax(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f3c48d-27a8-4ccb-ac01-2bf2247a5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UNet(in_channels=3, out_channels=8)\n",
    "# ins = torch.randn(2, 3, 400 ,400)\n",
    "# out = net(ins)\n",
    "# print(out.shape)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a108f7e-092d-4707-aded-ab0bac64587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network():\n",
    "    ins = torch.randn(2, 3, 800, 800).to(device)\n",
    "    net = UNet().to(device)\n",
    "    out = net(ins)\n",
    "    print(out.shape)\n",
    "    \n",
    "def dice_coeff(input, target, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert input.size() == target.size(), f\"{input.size()}, {target.size()}\"\n",
    "    assert input.dim() == 3 or not reduce_batch_first\n",
    "\n",
    "    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n",
    "\n",
    "    inter = 2 * (input * target).sum(dim=sum_dim)\n",
    "    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n",
    "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
    "\n",
    "    dice = (inter + epsilon) / (sets_sum + epsilon)\n",
    "    return dice.mean()\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(input, target, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n",
    "\n",
    "\n",
    "def dice_loss(input, target, multiclass: bool = False):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(input, target, reduce_batch_first=True)\n",
    "\n",
    "def pixel_accuracy(predictions, labels):\n",
    "    correct = (predictions == labels).float()\n",
    "    pacc = correct.sum() / correct.numel()\n",
    "    return pacc\n",
    "    \n",
    "def save_weight(name: str):\n",
    "        torch.save(net.state_dict(), os.path.join(WEIGHT_PATH, f\"checkpoint_{name}.weight\"))\n",
    "\n",
    "def load_weight(name: str):\n",
    "    print(\"load weight\", WEIGHT_PATH+name)\n",
    "    net.load_state_dict(torch.load(os.path.join(WEIGHT_PATH, f\"checkpoint_{name}.weight\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add5d41b-7f25-4431-8833-ae06034295e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-14676ce956ed>:2: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
      "/root/miniconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
    "    ]) \n",
    "trainset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"train\", first_transform=transform)\n",
    "trainLoader = DataLoader(trainset, batch_size=BATCH_SIZE, num_workers=NK, shuffle=True)\n",
    "\n",
    "valset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"val\", first_transform=transform)\n",
    "valLoader = DataLoader(valset, batch_size=BATCH_SIZE, num_workers=NK, shuffle=True)\n",
    "\n",
    "net = UNet(in_channels=3, out_channels=8, hiddens=[16, 32, 64, 128]).to(device)\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=LR)\n",
    "#optimizer = SGD(net.parameters(), lr=LR, momentum=0.9)\n",
    "log = {\"train_loss\": [], \"val_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3e99a29-9545-4cfa-a457-cad26f1ba028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    try:\n",
    "        for eps in range(EPOCHS):\n",
    "            net.train()\n",
    "            print(f\"{eps} epoch:\")\n",
    "            total_train_loss = 0\n",
    "            total_val_loss = 0\n",
    "            # train\n",
    "            print(\"train~\")\n",
    "            for ins, labels in tqdm(trainLoader):\n",
    "                ins_gpu = ins.to(device)\n",
    "                labels_gpu = labels.long().to(device)\n",
    "\n",
    "                pred = net(ins_gpu)\n",
    "                #pred = torch.argmax(pred, dim=1).unsqueeze(1).float()\n",
    "                #print(pred)\n",
    "                loss = criterion(pred, labels_gpu)\n",
    "                loss += dice_loss(F.softmax(pred, dim=1).float(), F.one_hot(labels_gpu, 8).permute(0, 3, 1, 2).float(), True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                #loss = ( 0.2 * criterion(pred, labels_gpu) + 0.8 * dice_loss(pred, labels_gpu) )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                #print(total_train_loss)\n",
    "\n",
    "            # val\n",
    "            print(\"val~\")\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                for ins, labels in tqdm(valLoader):\n",
    "                    ins_gpu = ins.to(device)\n",
    "                    labels_gpu = labels.long().to(device)\n",
    "\n",
    "                    pred = net(ins_gpu)\n",
    "                    loss = criterion(pred, labels_gpu)\n",
    "                    loss += dice_loss(F.softmax(pred, dim=1).float(), F.one_hot(labels_gpu, 8).permute(0, 3, 1, 2).float(), True)\n",
    "\n",
    "                    #loss = ( 0.2 * criterion(pred, labels_gpu) + 0.8 * dice_loss(pred, labels_gpu) )\n",
    "                    total_val_loss += loss.item()\n",
    "                    #total_val_loss += criterion(pred, labels_gpu).item()\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(trainLoader)\n",
    "            avg_val_loss = total_val_loss / len(valLoader)\n",
    "\n",
    "            if log[\"val_loss\"] and avg_val_loss < np.min(log[\"val_loss\"]):\n",
    "                print(\"save best weight\")\n",
    "                save_weight(\"best\")\n",
    "\n",
    "            log[\"train_loss\"].append(avg_train_loss)\n",
    "            log[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "            print(f\"avg_train_loss: {avg_train_loss}, avg_val_loss: {avg_val_loss}\")\n",
    "            if eps and eps%NUM_SAVE == 0:\n",
    "                save_weight(f\"{eps}\")\n",
    "                train_loss = np.array(log[\"train_loss\"])\n",
    "                val_loss = np.array(log[\"val_loss\"])\n",
    "                np.save(f\"{WEIGHT_PATH}/train_loss\", train_loss)\n",
    "                np.save(f\"{WEIGHT_PATH}/val_loss\", val_loss)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupt\")\n",
    "        save_weight(\"interrupt\")\n",
    "        train_loss = np.array(log[\"train_loss\"])\n",
    "        val_loss = np.array(log[\"val_loss\"])\n",
    "        np.save(f\"{WEIGHT_PATH}/train_loss\", train_loss)\n",
    "        np.save(f\"{WEIGHT_PATH}/val_loss\", val_loss)\n",
    "        \n",
    "    print(\"END\")\n",
    "    save_weight(\"END\")\n",
    "    train_loss = np.array(log[\"train_loss\"])\n",
    "    val_loss = np.array(log[\"val_loss\"])\n",
    "    np.save(f\"{WEIGHT_PATH}/train_loss\", train_loss)\n",
    "    np.save(f\"{WEIGHT_PATH}/val_loss\", val_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd119ec9-5b21-4c26-bdb8-80f60baee135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch:\n",
      "train~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 168/818 [01:36<05:55,  1.83it/s]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87fa7ffb-80ea-4ac1-909e-dee7635bdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(ins, gts, preds):\n",
    "    batch = ins.size()[0]\n",
    "    fig, axs = plt.subplots(3, batch, figsize=(100, 100))\n",
    "    for idx, (i, g, p) in enumerate(zip(ins, gts, preds)):\n",
    "        axs[0, idx].imshow(i.permute(1, 2, 0))\n",
    "        axs[1, idx].imshow(g)\n",
    "        axs[2, idx].imshow(p)\n",
    "    \n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "def test(show=False):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
    "    ]) \n",
    "    \n",
    "    testset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"test\", first_transform=transform)\n",
    "    testLoader = DataLoader(testset, batch_size=BATCH_SIZE, num_workers=NK, shuffle=True)\n",
    "    net = UNet(in_channels=3, out_channels=8, hiddens=[16, 32, 64, 128]).to(device)\n",
    "    load_weight(\"END\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        pa = 0\n",
    "        dice = 0\n",
    "        for ins, labels in tqdm(testLoader):\n",
    "            ins_gpu = ins.to(device)\n",
    "            #print(labels.shape)\n",
    "            labels_gpu = labels.long().to(device)\n",
    "            #print(labels_gpu.shape)\n",
    "            pred = net(ins_gpu)\n",
    "            #print(pred.shape, labels_gpu.shape)\n",
    "            test_loss = criterion(pred, labels_gpu).item()\n",
    "            dice += dice_loss(F.softmax(pred, dim=1).float(), F.one_hot(labels_gpu, 8).permute(0, 3, 1, 2).float(), True)\n",
    "            #print(\"test loss, \", test_loss)\n",
    "            #show_image(ins, labels, pred.cpu())\n",
    "            predictions = torch.nn.functional.softmax(pred, dim=1)\n",
    "            pred_labels = torch.argmax(predictions, dim=1) \n",
    "            pred_labels = pred_labels.float()\n",
    "            pa += pixel_accuracy(pred_labels, labels_gpu)\n",
    "            if show:\n",
    "                show_image(ins, labels, pred_labels.cpu())\n",
    "                break\n",
    "        if not show:\n",
    "            print(f\"pa: {pa/len(testLoader)}, dice: {dice/len(testLoader)}\")\n",
    "            #break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5bf528a-0232-4fcd-aa62-c1c6924ab89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-720833c57160>:14: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load weight task18best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [01:00<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa: 0.0001556072966195643, dice: 0.8750007748603821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4b9ba-3451-4673-aa3a-9c93b3ca04a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-e52261bf55d1>:14: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load weight task18END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/234 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "test(show=True)\n",
    "\n",
    "\"\"\" \n",
    "    this result show only 20 epoch \n",
    "    in report, it show 60 epoch's result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c42ea9a5-ef60-4f30-9792-41fc78d16aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = np.load(f\"{WEIGHT_PATH}/train_loss.npy\")\n",
    "val_loss = np.load(f\"{WEIGHT_PATH}/val_loss.npy\")\n",
    "\n",
    "plt.plot(train_loss, color=\"blue\", label=\"train\")\n",
    "plt.plot(val_loss, color=\"red\", label=\"val\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8e972-3bb3-4a8b-b8fe-dd405eb1ed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
