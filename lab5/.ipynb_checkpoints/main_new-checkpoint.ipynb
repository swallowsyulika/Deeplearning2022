{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9597f12a-ff40-496f-b247-9d5983e506ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, SGD\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355199a9-bf1a-4ab0-8f2f-4fa73cfde80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 2 NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.cuda.set_device(2)\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614307e8-96e5-47dd-888d-3485331dfc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"wg4bpm33hj-2/images\"\n",
    "MASKS_PATH = \"wg4bpm33hj-2/masks\"\n",
    "WEIGHT_PATH = \"task18\"\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = 800\n",
    "LR = 0.001\n",
    "NUM_SAVE = 5\n",
    "NK = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75266428-fb7a-496b-90ce-f96e5ce883ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCAgTDataset(Dataset):\n",
    "    def __init__(self, image_path: str, label_path: str, set_type=\"train\", first_transform=None, sec_transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.first_transform = first_transform\n",
    "        self.sec_transform = sec_transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.split_ratio = {\"train\": (0.0, 0.7), \"val\": (0.7, 0.8), \"test\": (0.8, 1.0)}\n",
    "        self.maxV = 7\n",
    "        self.mask_ratio = 255 // self.maxV\n",
    "        \n",
    "        assert set_type in self.split_ratio.keys(), \"dataset type error\"\n",
    "  \n",
    "        for root, dirs, files in os.walk(image_path):\n",
    "            for f in files:\n",
    "                self.images.append(os.path.join(root, f))\n",
    "                \n",
    "        for root, dirs, files in os.walk(label_path):\n",
    "            for f in files:\n",
    "                self.labels.append(os.path.join(root, f))\n",
    "                \n",
    "        assert len(self.images) == len(self.labels), f\"data length error, {len(self.images)}, {len(self.labels)}\"\n",
    "        \n",
    "        for ele in zip(sorted(self.images), sorted(self.labels)):\n",
    "            self.data.append(ele)\n",
    "        \n",
    "        shuffle(self.data)\n",
    "        self.data = self.data[int(len(self.data)*self.split_ratio[set_type][0]) : int(len(self.data)*self.split_ratio[set_type][1])]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, mask_path = self.data[index]\n",
    "\n",
    "        #image = torch.from_numpy(np.array(Image.open(image_path), dtype=np.float64))\n",
    "        image = Image.open(image_path)\n",
    "        #mask =  Image.open(mask_path).convert(\"L\")\n",
    "        mask =  Image.open(mask_path)\n",
    "        \n",
    "        if self.first_transform is not None:\n",
    "            image = self.first_transform(image)\n",
    "            y = self.first_transform(mask)\n",
    "\n",
    "        image = transforms.ToTensor()(image)\n",
    "        y = np.array(y)\n",
    "        y = torch.from_numpy(y)\n",
    "        \n",
    "        y = y.type(torch.LongTensor)\n",
    "        \n",
    "        return image, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db9af62-446f-4189-8c30-8826cca4f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#         transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
    "#     ]) \n",
    "# trainset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"train\", first_transform=transform)\n",
    "# x, y = trainset[0]\n",
    "# print(x.shape, y.shape)\n",
    "# print(x)\n",
    "# print(torch.unique(y))\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e00ad95-dba7-4f99-a5b3-7e5d234fe5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = np.random.randint(0, 8, (8, 1, 3, 3))\n",
    "# z = torch.randint(0, 8, (8, 8, 3, 3)).float()\n",
    "# #y = torch.clone(z)\n",
    "# #print(y)\n",
    "# n = np.zeros((8, 8, 3, 3))\n",
    "\n",
    "# for i, row in enumerate(y):\n",
    "#     for j, ele in enumerate(row):\n",
    "#         print(ele)\n",
    "#         n[ele][i][j] = 1\n",
    "# print(n)\n",
    "\n",
    "# sf = nn.Softmax(1)\n",
    "# #print(sf(z))\n",
    "# print(z[0])\n",
    "# x = torch.argmax(z, axis=1).unsqueeze(1).float()\n",
    "# print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f4ac2c-cd04-4240-a71c-f5fb82c61783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "#         transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "#     ])\n",
    "# label_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "#     ])\n",
    "# trainset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"train\", image_transform=image_transform, label_transform=label_transform)\n",
    "# x, y = trainset[0]\n",
    "# y = y.int()\n",
    "# print(torch.max(y))\n",
    "# print(torch.min(y))\n",
    "# print(y.shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd55d0b-d3ae-4c17-be48-4b9a524a0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up_layer = nn.ConvTranspose2d(in_channels, out_channels, 2, 2)\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, keep):\n",
    "        x = self.up_layer(x)\n",
    "        x = torch.cat([x, keep], dim=1)\n",
    "        x = self.double_conv(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, hiddens=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList([Up(hidden*2, hidden) for hidden in hiddens[::-1]])\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        for hidden in hiddens:\n",
    "            self.downs.append(DoubleConv(in_channels, hidden))\n",
    "            in_channels = hidden\n",
    "            \n",
    "        #for hidden in hiddens[::-1]:\n",
    "        #    self.ups.append(Up(hidden*2, hidden))\n",
    "            \n",
    "        self.midden_layer = DoubleConv(hiddens[-1], hiddens[-1]*2)\n",
    "        self.out_layer = nn.Conv2d(hiddens[0], out_channels, 1)\n",
    "        #self.sofmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keeps = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            keeps.append(x)\n",
    "            x = self.pool(x)\n",
    "            \n",
    "        x = self.midden_layer(x)\n",
    "        keeps = keeps[::-1]\n",
    "        \n",
    "        for i, up in enumerate(self.ups):\n",
    "            x = up(x, keeps[i])\n",
    "            \n",
    "        x = self.out_layer(x)\n",
    "        #x = self.sofmax(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f3c48d-27a8-4ccb-ac01-2bf2247a5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UNet(in_channels=3, out_channels=8)\n",
    "# ins = torch.randn(2, 3, 400 ,400)\n",
    "# out = net(ins)\n",
    "# print(out.shape)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a108f7e-092d-4707-aded-ab0bac64587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network():\n",
    "    ins = torch.randn(2, 3, 800, 800).to(device)\n",
    "    net = UNet().to(device)\n",
    "    out = net(ins)\n",
    "    print(out.shape)\n",
    "    \n",
    "def dice_coeff(input, target, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert input.size() == target.size(), f\"{input.size()}, {target.size()}\"\n",
    "    assert input.dim() == 3 or not reduce_batch_first\n",
    "\n",
    "    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n",
    "\n",
    "    inter = 2 * (input * target).sum(dim=sum_dim)\n",
    "    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n",
    "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
    "\n",
    "    dice = (inter + epsilon) / (sets_sum + epsilon)\n",
    "    return dice.mean()\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(input, target, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n",
    "\n",
    "\n",
    "def dice_loss(input, target, multiclass: bool = False):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(input, target, reduce_batch_first=True)\n",
    "\n",
    "def pixel_accuracy(predictions, labels):\n",
    "    correct = (predictions == labels).float()\n",
    "    pacc = correct.sum() / correct.numel()\n",
    "    return pacc\n",
    "    \n",
    "def save_weight(name: str):\n",
    "        torch.save(net.state_dict(), os.path.join(WEIGHT_PATH, f\"checkpoint_{name}.weight\"))\n",
    "\n",
    "def load_weight(name: str):\n",
    "    print(\"load weight\", WEIGHT_PATH+name)\n",
    "    net.load_state_dict(torch.load(os.path.join(WEIGHT_PATH, f\"checkpoint_{name}.weight\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add5d41b-7f25-4431-8833-ae06034295e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-14676ce956ed>:2: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
      "/root/miniconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
    "    ]) \n",
    "trainset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"train\", first_transform=transform)\n",
    "trainLoader = DataLoader(trainset, batch_size=BATCH_SIZE, num_workers=NK, shuffle=True)\n",
    "\n",
    "valset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"val\", first_transform=transform)\n",
    "valLoader = DataLoader(valset, batch_size=BATCH_SIZE, num_workers=NK, shuffle=True)\n",
    "\n",
    "net = UNet(in_channels=3, out_channels=8, hiddens=[16, 32, 64, 128]).to(device)\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=LR)\n",
    "#optimizer = SGD(net.parameters(), lr=LR, momentum=0.9)\n",
    "log = {\"train_loss\": [], \"val_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3e99a29-9545-4cfa-a457-cad26f1ba028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    try:\n",
    "        for eps in range(EPOCHS):\n",
    "            net.train()\n",
    "            print(f\"{eps} epoch:\")\n",
    "            total_train_loss = 0\n",
    "            total_val_loss = 0\n",
    "            # train\n",
    "            print(\"train~\")\n",
    "            for ins, labels in tqdm(trainLoader):\n",
    "                ins_gpu = ins.to(device)\n",
    "                labels_gpu = labels.long().to(device)\n",
    "\n",
    "                pred = net(ins_gpu)\n",
    "                #pred = torch.argmax(pred, dim=1).unsqueeze(1).float()\n",
    "                #print(pred)\n",
    "                loss = criterion(pred, labels_gpu)\n",
    "                loss += dice_loss(F.softmax(pred, dim=1).float(), F.one_hot(labels_gpu, 8).permute(0, 3, 1, 2).float(), True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                #loss = ( 0.2 * criterion(pred, labels_gpu) + 0.8 * dice_loss(pred, labels_gpu) )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                #print(total_train_loss)\n",
    "\n",
    "            # val\n",
    "            print(\"val~\")\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                for ins, labels in tqdm(valLoader):\n",
    "                    ins_gpu = ins.to(device)\n",
    "                    labels_gpu = labels.long().to(device)\n",
    "\n",
    "                    pred = net(ins_gpu)\n",
    "                    loss = criterion(pred, labels_gpu)\n",
    "                    loss += dice_loss(F.softmax(pred, dim=1).float(), F.one_hot(labels_gpu, 8).permute(0, 3, 1, 2).float(), True)\n",
    "\n",
    "                    #loss = ( 0.2 * criterion(pred, labels_gpu) + 0.8 * dice_loss(pred, labels_gpu) )\n",
    "                    total_val_loss += loss.item()\n",
    "                    #total_val_loss += criterion(pred, labels_gpu).item()\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(trainLoader)\n",
    "            avg_val_loss = total_val_loss / len(valLoader)\n",
    "\n",
    "            if log[\"val_loss\"] and avg_val_loss < np.min(log[\"val_loss\"]):\n",
    "                print(\"save best weight\")\n",
    "                save_weight(\"best\")\n",
    "\n",
    "            log[\"train_loss\"].append(avg_train_loss)\n",
    "            log[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "            print(f\"avg_train_loss: {avg_train_loss}, avg_val_loss: {avg_val_loss}\")\n",
    "            if eps and eps%NUM_SAVE == 0:\n",
    "                save_weight(f\"{eps}\")\n",
    "                train_loss = np.array(log[\"train_loss\"])\n",
    "                val_loss = np.array(log[\"val_loss\"])\n",
    "                np.save(f\"{WEIGHT_PATH}/train_loss\", train_loss)\n",
    "                np.save(f\"{WEIGHT_PATH}/val_loss\", val_loss)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupt\")\n",
    "        save_weight(\"interrupt\")\n",
    "        train_loss = np.array(log[\"train_loss\"])\n",
    "        val_loss = np.array(log[\"val_loss\"])\n",
    "        np.save(f\"{WEIGHT_PATH}/train_loss\", train_loss)\n",
    "        np.save(f\"{WEIGHT_PATH}/val_loss\", val_loss)\n",
    "        \n",
    "    print(\"END\")\n",
    "    save_weight(\"END\")\n",
    "    train_loss = np.array(log[\"train_loss\"])\n",
    "    val_loss = np.array(log[\"val_loss\"])\n",
    "    np.save(f\"{WEIGHT_PATH}/train_loss\", train_loss)\n",
    "    np.save(f\"{WEIGHT_PATH}/val_loss\", val_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd119ec9-5b21-4c26-bdb8-80f60baee135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch:\n",
      "train~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 168/818 [01:36<05:55,  1.83it/s]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa7ffb-80ea-4ac1-909e-dee7635bdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(ins, gts, preds):\n",
    "    batch = ins.size()[0]\n",
    "    fig, axs = plt.subplots(3, batch, figsize=(100, 100))\n",
    "    for idx, (i, g, p) in enumerate(zip(ins, gts, preds)):\n",
    "        axs[0, idx].imshow(i.permute(1, 2, 0))\n",
    "        axs[1, idx].imshow(g)\n",
    "        axs[2, idx].imshow(p)\n",
    "    \n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "def test(show=False):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
    "    ]) \n",
    "    \n",
    "    testset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"test\", first_transform=transform)\n",
    "    testLoader = DataLoader(testset, batch_size=BATCH_SIZE, num_workers=NK, shuffle=True)\n",
    "    net = UNet(in_channels=3, out_channels=8, hiddens=[16, 32, 64, 128]).to(device)\n",
    "    load_weight(\"best\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        pa = 0\n",
    "        dice = 0\n",
    "        for ins, labels in tqdm(testLoader):\n",
    "            ins_gpu = ins.to(device)\n",
    "            #print(labels.shape)\n",
    "            labels_gpu = labels.long().to(device)\n",
    "            #print(labels_gpu.shape)\n",
    "            pred = net(ins_gpu)\n",
    "            #print(pred.shape, labels_gpu.shape)\n",
    "            test_loss = criterion(pred, labels_gpu).item()\n",
    "            dice += dice_loss(F.softmax(pred, dim=1).float(), F.one_hot(labels_gpu, 8).permute(0, 3, 1, 2).float(), True)\n",
    "            #print(\"test loss, \", test_loss)\n",
    "            #show_image(ins, labels, pred.cpu())\n",
    "            predictions = torch.nn.functional.softmax(pred, dim=1)\n",
    "            pred_labels = torch.argmax(predictions, dim=1) \n",
    "            pred_labels = pred_labels.float()\n",
    "            pa += pixel_accuracy(pred_labels, labels_gpu)\n",
    "            if show:\n",
    "                show_image(ins, labels, pred_labels.cpu())\n",
    "                break\n",
    "        if not show:\n",
    "            print(f\"pa: {pa/len(testLoader)}, dice: {dice/len(testLoader)}\")\n",
    "            #break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf528a-0232-4fcd-aa62-c1c6924ab89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4b9ba-3451-4673-aa3a-9c93b3ca04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(show=True)\n",
    "\n",
    "\"\"\" \n",
    "    this result show only 20 epoch \n",
    "    in report, it show 60 epoch's result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ea9a5-ef60-4f30-9792-41fc78d16aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.load(f\"{WEIGHT_PATH}/train_loss.npy\")\n",
    "val_loss = np.load(f\"{WEIGHT_PATH}/val_loss.npy\")\n",
    "\n",
    "plt.plot(train_loss, color=\"blue\", label=\"train\")\n",
    "plt.plot(val_loss, color=\"red\", label=\"val\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8e972-3bb3-4a8b-b8fe-dd405eb1ed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
