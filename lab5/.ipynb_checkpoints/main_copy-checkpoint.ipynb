{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9597f12a-ff40-496f-b247-9d5983e506ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, SGD\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355199a9-bf1a-4ab0-8f2f-4fa73cfde80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 0 NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614307e8-96e5-47dd-888d-3485331dfc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"wg4bpm33hj-2/images\"\n",
    "MASKS_PATH = \"wg4bpm33hj-2/masks\"\n",
    "WEIGHT_PATH = \"task16\"\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = 400\n",
    "LR = 0.001\n",
    "NUM_SAVE = 5\n",
    "NK = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75266428-fb7a-496b-90ce-f96e5ce883ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCAgTDataset(Dataset):\n",
    "    def __init__(self, image_path: str, label_path: str, set_type=\"train\", first_transform=None, sec_transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.first_transform = first_transform\n",
    "        self.sec_transform = sec_transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.split_ratio = {\"train\": (0.0, 0.7), \"val\": (0.7, 0.8), \"test\": (0.8, 1.0)}\n",
    "        self.maxV = 7\n",
    "        self.mask_ratio = 255 // self.maxV\n",
    "        \n",
    "        assert set_type in self.split_ratio.keys(), \"dataset type error\"\n",
    "  \n",
    "        for root, dirs, files in os.walk(image_path):\n",
    "            for f in files:\n",
    "                self.images.append(os.path.join(root, f))\n",
    "                \n",
    "        for root, dirs, files in os.walk(label_path):\n",
    "            for f in files:\n",
    "                self.labels.append(os.path.join(root, f))\n",
    "                \n",
    "        assert len(self.images) == len(self.labels), f\"data length error, {len(self.images)}, {len(self.labels)}\"\n",
    "        \n",
    "        for ele in zip(sorted(self.images), sorted(self.labels)):\n",
    "            self.data.append(ele)\n",
    "        \n",
    "        shuffle(self.data)\n",
    "        self.data = self.data[int(len(self.data)*self.split_ratio[set_type][0]) : int(len(self.data)*self.split_ratio[set_type][1])]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, mask_path = self.data[index]\n",
    "\n",
    "        #image = torch.from_numpy(np.array(Image.open(image_path), dtype=np.float64))\n",
    "        image = Image.open(image_path)\n",
    "        #mask =  Image.open(mask_path).convert(\"L\")\n",
    "        mask =  Image.open(mask_path)\n",
    "        \n",
    "        if self.first_transform is not None:\n",
    "            image = self.first_transform(image)\n",
    "            y = self.first_transform(mask)\n",
    "\n",
    "        image = transforms.ToTensor()(image)\n",
    "        y = np.array(y)\n",
    "        y = torch.from_numpy(y)\n",
    "        \n",
    "        y = y.type(torch.LongTensor)\n",
    "        \n",
    "        return image, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e00ad95-dba7-4f99-a5b3-7e5d234fe5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = np.random.randint(0, 8, (8, 1, 3, 3))\n",
    "# z = torch.randint(0, 8, (8, 8, 3, 3)).float()\n",
    "# #y = torch.clone(z)\n",
    "# #print(y)\n",
    "# n = np.zeros((8, 8, 3, 3))\n",
    "\n",
    "# for i, row in enumerate(y):\n",
    "#     for j, ele in enumerate(row):\n",
    "#         print(ele)\n",
    "#         n[ele][i][j] = 1\n",
    "# print(n)\n",
    "\n",
    "# sf = nn.Softmax(1)\n",
    "# #print(sf(z))\n",
    "# print(z[0])\n",
    "# x = torch.argmax(z, axis=1).unsqueeze(1).float()\n",
    "# print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f4ac2c-cd04-4240-a71c-f5fb82c61783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "#         transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "#     ])\n",
    "# label_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "#     ])\n",
    "# trainset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"train\", image_transform=image_transform, label_transform=label_transform)\n",
    "# x, y = trainset[0]\n",
    "# y = y.int()\n",
    "# print(torch.max(y))\n",
    "# print(torch.min(y))\n",
    "# print(y.shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd55d0b-d3ae-4c17-be48-4b9a524a0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f3c48d-27a8-4ccb-ac01-2bf2247a5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UNet(in_channels=3, out_channels=8)\n",
    "# ins = torch.randn(2, 3, 400 ,400)\n",
    "# out = net(ins)\n",
    "# print(out.shape)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a108f7e-092d-4707-aded-ab0bac64587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network():\n",
    "    ins = torch.randn(2, 3, 800, 800).to(device)\n",
    "    net = UNet().to(device)\n",
    "    out = net(ins)\n",
    "    print(out.shape)\n",
    "    \n",
    "def dice_coeff(input, target, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert input.size() == target.size()\n",
    "    assert input.dim() == 3 or not reduce_batch_first\n",
    "\n",
    "    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n",
    "\n",
    "    inter = 2 * (input * target).sum(dim=sum_dim)\n",
    "    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n",
    "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
    "\n",
    "    dice = (inter + epsilon) / (sets_sum + epsilon)\n",
    "    return dice.mean()\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(input, target, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n",
    "\n",
    "\n",
    "def dice_loss(input, target, multiclass: bool = False):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(input, target, reduce_batch_first=True)\n",
    "    \n",
    "def save_weight(name: str):\n",
    "        torch.save(net.state_dict(), os.path.join(WEIGHT_PATH, f\"checkpoint_{name}.weight\"))\n",
    "\n",
    "def load_weight(name: str):\n",
    "    net.load_state_dict(torch.load(os.path.join(WEIGHT_PATH, f\"checkpoint_{name}.weight\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "add5d41b-7f25-4431-8833-ae06034295e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-c26103ad7be6>:2: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
      "/root/miniconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
    "    ]) \n",
    "trainset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"train\", first_transform=transform)\n",
    "trainLoader = DataLoader(trainset, batch_size=BATCH_SIZE, num_workers=NK, shuffle=True)\n",
    "\n",
    "valset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"val\", first_transform=transform)\n",
    "valLoader = DataLoader(valset, batch_size=BATCH_SIZE, num_workers=NK, shuffle=True)\n",
    "\n",
    "net = UNet(3, 8).to(device)\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=LR)\n",
    "#optimizer = SGD(net.parameters(), lr=LR, momentum=0.9)\n",
    "log = {\"train_loss\": [], \"val_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84950950-0676-4126-8a37-62a3b04f97c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8314, 0.8510, 0.8588,  ..., 0.7765, 0.7843, 0.8000],\n",
       "         [0.8275, 0.8431, 0.8510,  ..., 0.7961, 0.7882, 0.7882],\n",
       "         [0.8275, 0.8392, 0.8471,  ..., 0.8039, 0.8000, 0.7804],\n",
       "         ...,\n",
       "         [0.8275, 0.8196, 0.8314,  ..., 0.8275, 0.8275, 0.8235],\n",
       "         [0.8314, 0.8275, 0.8275,  ..., 0.8275, 0.8353, 0.8275],\n",
       "         [0.8275, 0.8392, 0.8353,  ..., 0.8353, 0.8353, 0.8353]],\n",
       "\n",
       "        [[0.8157, 0.8353, 0.8392,  ..., 0.7412, 0.7490, 0.7529],\n",
       "         [0.8157, 0.8235, 0.8392,  ..., 0.7490, 0.7529, 0.7529],\n",
       "         [0.8157, 0.8196, 0.8353,  ..., 0.7608, 0.7569, 0.7529],\n",
       "         ...,\n",
       "         [0.8118, 0.8078, 0.8039,  ..., 0.8118, 0.8235, 0.8275],\n",
       "         [0.8196, 0.8157, 0.8157,  ..., 0.8118, 0.8196, 0.8235],\n",
       "         [0.8157, 0.8118, 0.8196,  ..., 0.8118, 0.8196, 0.8196]],\n",
       "\n",
       "        [[0.7804, 0.8000, 0.8275,  ..., 0.6824, 0.6902, 0.6980],\n",
       "         [0.7882, 0.8000, 0.8196,  ..., 0.6863, 0.6863, 0.6863],\n",
       "         [0.7961, 0.8039, 0.8078,  ..., 0.6902, 0.6863, 0.6784],\n",
       "         ...,\n",
       "         [0.7765, 0.7804, 0.7647,  ..., 0.8157, 0.8078, 0.8078],\n",
       "         [0.7922, 0.7882, 0.7882,  ..., 0.8078, 0.8078, 0.8078],\n",
       "         [0.7961, 0.7804, 0.7765,  ..., 0.8118, 0.8078, 0.8078]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3e99a29-9545-4cfa-a457-cad26f1ba028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    try:\n",
    "        for eps in range(EPOCHS):\n",
    "            net.train()\n",
    "            print(f\"{eps} epoch:\")\n",
    "            total_train_loss = 0\n",
    "            total_val_loss = 0\n",
    "            # train\n",
    "            print(\"train~\")\n",
    "            for ins, labels in tqdm(trainLoader):\n",
    "                ins_gpu = ins.to(device)\n",
    "                labels_gpu = labels.long().squeeze(1).to(device)\n",
    "\n",
    "                pred = net(ins_gpu)\n",
    "                #pred = torch.argmax(pred, dim=1).unsqueeze(1).float()\n",
    "                #print(pred)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(pred, labels_gpu)\n",
    "                loss += dice_loss(F.softmax(pred, dim=1).float(), F.one_hot(labels_gpu, 8).permute(0, 3, 1, 2).float(), True)\n",
    "\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                #print(total_train_loss)\n",
    "\n",
    "            # val\n",
    "            print(\"val~\")\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                for ins, labels in tqdm(valLoader):\n",
    "                    ins_gpu = ins.to(device)\n",
    "                    labels_gpu = labels.long().squeeze(1).to(device)\n",
    "\n",
    "                    pred = net(ins_gpu)\n",
    "                    loss = criterion(pred, labels_gpu)\n",
    "                    loss += dice_loss(F.softmax(pred, dim=1).float(), F.one_hot(labels_gpu, 8).permute(0, 3, 1, 2).float(), True)\n",
    "                    total_val_loss += loss.item()\n",
    "                    #total_val_loss += criterion(pred, labels_gpu).item()\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(trainLoader)\n",
    "            avg_val_loss = total_val_loss / len(valLoader)\n",
    "\n",
    "            if log[\"val_loss\"] and avg_val_loss < np.min(log[\"val_loss\"]):\n",
    "                print(\"save best weight\")\n",
    "                save_weight(\"best\")\n",
    "\n",
    "            log[\"train_loss\"].append(avg_train_loss)\n",
    "            log[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "            print(f\"avg_train_loss: {avg_train_loss}, avg_val_loss: {avg_val_loss}\")\n",
    "            if eps and eps%NUM_SAVE == 0:\n",
    "                save_weight(f\"{eps}\")\n",
    "                train_loss = np.array(log[\"train_loss\"])\n",
    "                val_loss = np.array(log[\"val_loss\"])\n",
    "                np.save(f\"{WEIGHT_PATH}/train_loss\", train_loss)\n",
    "                np.save(f\"{WEIGHT_PATH}/val_loss\", val_loss)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupt\")\n",
    "        save_weight(\"interrupt\")\n",
    "        train_loss = np.array(log[\"train_loss\"])\n",
    "        val_loss = np.array(log[\"val_loss\"])\n",
    "        np.save(f\"{WEIGHT_PATH}/train_loss\", train_loss)\n",
    "        np.save(f\"{WEIGHT_PATH}/val_loss\", val_loss)\n",
    "        \n",
    "    print(\"END\")\n",
    "    save_weight(\"END\")\n",
    "    train_loss = np.array(log[\"train_loss\"])\n",
    "    val_loss = np.array(log[\"val_loss\"])\n",
    "    np.save(f\"{WEIGHT_PATH}/train_loss\", train_loss)\n",
    "    np.save(f\"{WEIGHT_PATH}/val_loss\", val_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd119ec9-5b21-4c26-bdb8-80f60baee135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch:\n",
      "train~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 818/818 [09:30<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:30<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss: 0.13853674180222753, avg_val_loss: 0.04278591357999378\n",
      "1 epoch:\n",
      "train~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 818/818 [09:32<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:29<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save best weight\n",
      "avg_train_loss: 0.03797403323489144, avg_val_loss: 0.03695838946180466\n",
      "2 epoch:\n",
      "train~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 818/818 [09:32<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:30<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss: 0.03384473460241424, avg_val_loss: 0.03819139381377106\n",
      "3 epoch:\n",
      "train~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 818/818 [09:33<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:30<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save best weight\n",
      "avg_train_loss: 0.030451005902528434, avg_val_loss: 0.03386809368036751\n",
      "4 epoch:\n",
      "train~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 818/818 [09:35<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:30<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save best weight\n",
      "avg_train_loss: 0.027114422982194276, avg_val_loss: 0.027816891339886136\n",
      "5 epoch:\n",
      "train~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 768/818 [08:58<00:34,  1.43it/s]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa7ffb-80ea-4ac1-909e-dee7635bdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image(mask, threshold=0.2):\n",
    "    ch, h, w = mask.size()\n",
    "    img = np.zeros((h, w, 1))\n",
    "    for idx, channel in enumerate(mask):\n",
    "        img[:][channel>threshold] = idx\n",
    "#         for i, row in enumerate(channel):\n",
    "#             for j, ele in enumerate(row):\n",
    "#                 if ele > threshold:\n",
    "#                     img[i][j] = idx\n",
    "                    \n",
    "    print(np.max(img))\n",
    "    print(np.min(img))\n",
    "    print(img.shape)\n",
    "\n",
    "def show_image(ins, gts, preds):\n",
    "    print(torch.max(preds))\n",
    "    print(torch.min(preds))\n",
    "    predictions = torch.nn.functional.softmax(preds, dim=1)\n",
    "    pred_labels = torch.argmax(predictions, dim=1) \n",
    "    pred_labels = pred_labels.float()\n",
    "    batch = ins.size()[0]\n",
    "    fig, axs = plt.subplots(3, batch, constrained_layout=True, figsize=(100, 100))\n",
    "    for idx, (i, g, p) in enumerate(zip(ins, gts, pred_labels)):\n",
    "        #print(i.shape, g.shape, p.shape)\n",
    "        axs[0, idx].imshow(i.permute(1, 2, 0))\n",
    "        axs[1, idx].imshow(g.permute(1, 2, 0))\n",
    "        axs[2, idx].imshow(p)\n",
    "    \n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "def test():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
    "    ]) \n",
    "    \n",
    "    testset = CCAgTDataset(IMAGES_PATH, MASKS_PATH, \"test\", first_transform=transform)\n",
    "    testLoader = DataLoader(testset, batch_size=BATCH_SIZE, num_workers=NK, shuffle=True)\n",
    "    #net = UNet(3, 8).to(device)\n",
    "    #load_weight(\"best\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for ins, labels in tqdm(testLoader):\n",
    "            ins_gpu = ins.to(device)\n",
    "            labels_gpu = labels.long().squeeze(1).to(device)\n",
    "\n",
    "            pred = net(ins_gpu)\n",
    "            #print(pred.shape, labels_gpu.shape)\n",
    "            test_loss = criterion(pred, labels_gpu).item()\n",
    "            print(\"test loss, \", test_loss)\n",
    "            show_image(ins, labels, pred.cpu())\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf528a-0232-4fcd-aa62-c1c6924ab89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ea9a5-ef60-4f30-9792-41fc78d16aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.load(f\"{WEIGHT_PATH}/train_loss.npy\")\n",
    "val_loss = np.load(f\"{WEIGHT_PATH}/val_loss.npy\")\n",
    "\n",
    "plt.plot(train_loss, color=\"blue\")\n",
    "plt.plot(val_loss, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9adff-1faf-4232-ac47-1c1aa0ed0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(predictions, labels):\n",
    "    correct = (predictions == labels).float()\n",
    "    pacc = correct.sum() / correct.numel()\n",
    "    return pacc\n",
    "\n",
    "def class_pixel_accuracy(predictions, labels, num_classes):\n",
    "    correct = (predictions == labels).float()\n",
    "    class_correct = [correct[labels == i].sum() for i in range(num_classes)]\n",
    "    class_total = [labels[labels == i].numel() for i in range(num_classes)]\n",
    "    class_acc = [c / t for c, t in zip(class_correct, class_total)]\n",
    "    return sum(class_acc) / num_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8e972-3bb3-4a8b-b8fe-dd405eb1ed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
